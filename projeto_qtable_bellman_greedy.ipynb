{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class DeliveryEnvironment:\n",
    "    def __init__(self, width, height, obstacles, deliveries):\n",
    "        \"\"\"\n",
    "        Inicializa o ambiente de entrega, definindo suas dimensões, obstáculos e pontos de entrega.\n",
    "        \n",
    "        :param width: Inteiro representando a largura da grade do ambiente.\n",
    "        :param height: Inteiro representando a altura da grade do ambiente.\n",
    "        :param obstacles: Lista de tuplas, onde cada tupla contém as coordenadas (x, y) de um obstáculo na grade.\n",
    "        :param deliveries: Lista de tuplas, onde cada tupla contém as coordenadas (x, y) de um ponto de entrega na grade.\n",
    "        \"\"\"\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.obstacles = obstacles\n",
    "        self.deliveries = deliveries\n",
    "        self.grid = self.initialize_grid()  # Cria a representação da grade com base nos obstáculos e entregas\n",
    "        self.agent_position = (4, 4)  # Posição inicial do agente no canto superior esquerdo da grade.\n",
    "\n",
    "    def initialize_grid(self):\n",
    "        \"\"\"\n",
    "        Cria e retorna uma matriz representando a grade do ambiente, marcando obstáculos e pontos de entrega.\n",
    "        \n",
    "        :return: Uma matriz 2D onde cada célula pode ser 0 (espaço livre), -1 (obstáculo) ou 1 (ponto de entrega).\n",
    "        \"\"\"\n",
    "        # Inicializa uma grade 2D com zeros (espaços livres).\n",
    "        grid = [[0 for _ in range(self.width)] for _ in range(self.height)]\n",
    "        # Marca os obstáculos na grade com -1.\n",
    "        for obstacle in self.obstacles:\n",
    "            grid[obstacle[1]][obstacle[0]] = -1\n",
    "        # Marca os pontos de entrega na grade com 1.\n",
    "        for delivery in self.deliveries:\n",
    "            grid[delivery[1]][delivery[0]] = 1\n",
    "        return grid\n",
    "\n",
    "    def move_agent(self, action):\n",
    "        \"\"\"\n",
    "        Move o agente na grade com base na ação escolhida, se possível.\n",
    "        \n",
    "        :param action: Inteiro representando a ação a ser tomada pelo agente (0: cima, 1: baixo, 2: esquerda, 3: direita).\n",
    "        :return: A recompensa associada ao novo estado do agente após a movimentação ou uma penalidade se o movimento for inválido.\n",
    "        \"\"\"\n",
    "        # Mapeia a ação escolhida para um deslocamento na grade.\n",
    "        moves = {0: (-1, 0), 1: (1, 0), 2: (0, -1), 3: (0, 1)}\n",
    "        move = moves[action]\n",
    "        # Calcula a nova posição do agente com base na ação.\n",
    "        new_position = (self.agent_position[0] + move[0], self.agent_position[1] + move[1])\n",
    "\n",
    "        # Verifica se a nova posição é válida (dentro dos limites da grade e não é um obstáculo).\n",
    "        if 0 <= new_position[0] < self.height and 0 <= new_position[1] < self.width and self.grid[new_position[0]][new_position[1]] != -1:\n",
    "            self.agent_position = new_position  # Atualiza a posição do agente.\n",
    "            return self.grid[new_position[0]][new_position[1]]  # Retorna a recompensa baseada no estado alcançado.\n",
    "        else:\n",
    "            return -1  # Retorna uma penalidade se a ação levar a uma posição inválida.\n",
    "\n",
    "    def is_delivery_completed(self):\n",
    "        \"\"\"\n",
    "        Verifica se o agente alcançou um ponto de entrega.\n",
    "        \n",
    "        :return: Booleano indicando se o agente está em um ponto de entrega.\n",
    "        \"\"\"\n",
    "        # Retorna True se a posição atual do agente é um ponto de entrega, caso contrário False.\n",
    "        return self.grid[self.agent_position[0]][self.agent_position[1]] == 1\n",
    "    \n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reinicia o ambiente e a posição do agente para o estado inicial.\n",
    "        \"\"\"\n",
    "        # Define a posição do agente de volta ao ponto de partida.\n",
    "        self.agent_position = (4, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(self, environment, learning_rate=0.1, discount_factor=0.95, epsilon=0.1):\n",
    "        \"\"\"\n",
    "        Inicializa o agente de Q-learning.\n",
    "        \n",
    "        :param environment: O ambiente no qual o agente opera.\n",
    "        :param learning_rate: Taxa de aprendizado (alfa), que determina o quanto da nova informação substitui a antiga.\n",
    "        :param discount_factor: Fator de desconto (gamma), que pondera a importância das recompensas futuras.\n",
    "        :param epsilon: Parâmetro para a política epsilon-greedy, equilibrando exploração e explotação.\n",
    "        \"\"\"\n",
    "        self.environment = environment\n",
    "        self.learning_rate = learning_rate\n",
    "        self.discount_factor = discount_factor\n",
    "        self.epsilon = epsilon\n",
    "        self.q_table = np.zeros((environment.height, environment.width, 4))\n",
    "        self.path = []  # Inicializa a lista para armazenar o caminho percorrido pelo agente\n",
    "\n",
    "    def choose_action(self):\n",
    "        \"\"\"\n",
    "        Escolhe uma ação usando a política epsilon-greedy.\n",
    "        \n",
    "        Com uma probabilidade epsilon, escolhe uma ação aleatória (exploração).\n",
    "        Com a probabilidade restante, escolhe a melhor ação conhecida (explotação).\n",
    "        \"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Exploração: escolhe uma ação aleatoriamente.\n",
    "            return np.random.randint(4)\n",
    "        else:\n",
    "            # Explotação: escolhe a melhor ação conhecida baseada na Q-Table.\n",
    "            state = self.environment.agent_position\n",
    "            return np.argmax(self.q_table[state[0], state[1]])\n",
    "\n",
    "    def update_q_table(self, reward, action, current_state, next_state):\n",
    "        \"\"\"\n",
    "        Atualiza a Q-Table usando a equação de Bellman.\n",
    "        \n",
    "        :param reward: A recompensa recebida após tomar a ação.\n",
    "        :param action: A ação que foi tomada.\n",
    "        :param current_state: O estado do agente antes da ação ser tomada.\n",
    "        :param next_state: O estado do agente após a ação ser tomada.\n",
    "        \"\"\"\n",
    "        # Calcula o melhor valor Q futuro para o próximo estado\n",
    "        future_rewards = np.max(self.q_table[next_state[0], next_state[1]])\n",
    "        \n",
    "        # Atualiza o valor Q para o estado atual e ação tomada\n",
    "        self.q_table[current_state[0], current_state[1], action] += self.learning_rate * (\n",
    "            reward + self.discount_factor * future_rewards - self.q_table[current_state[0], current_state[1], action]\n",
    "        )\n",
    "\n",
    "\n",
    "    def train(self, episodes=1000):\n",
    "        for episode in range(episodes):\n",
    "            self.environment.reset()  # Reinicia o ambiente ao estado inicial\n",
    "            done = False\n",
    "            # self.path = []  # Limpa o caminho no início de cada episódio\n",
    "\n",
    "            while not done:\n",
    "                current_position = self.environment.agent_position  # Posição atual antes da ação\n",
    "                action = self.choose_action()  # Escolhe a ação\n",
    "                reward = self.environment.move_agent(action)  # Executa a ação\n",
    "                next_position = self.environment.agent_position  # Posição após a ação\n",
    "\n",
    "                # Verifica se a entrega foi completada antes de aplicar a penalidade por viver\n",
    "                if not self.environment.is_delivery_completed():\n",
    "                    reward -= 0.1  # Penalidade por viver aplicada somente se a entrega não for completada\n",
    "\n",
    "                # Atualiza a Q-Table, passando os estados atual e futuro corretamente\n",
    "                self.update_q_table(reward, action, current_position, next_position)\n",
    "\n",
    "                print(f\"Época {episode + 1}:\")\n",
    "                print(f\"Estado anterior: {current_position}\")\n",
    "                print(f\"Ação tomada pelo agente: {action}\")\n",
    "                print(f\"Estado atual: {next_position}\")\n",
    "                print(f\"Recompensa: {reward}\\n\")\n",
    "\n",
    "                # Verifica se a entrega foi completada para encerrar o loop\n",
    "                done = self.environment.is_delivery_completed()\n",
    "\n",
    "                # Atualiza o caminho\n",
    "                self.path.append(current_position)  # Adiciona a posição atual ao caminho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de criação do ambiente e do agente\n",
    "environment = DeliveryEnvironment(9, 9, [(1, 6), (2, 6), (2, 7), (6, 1), (6, 2), (7, 2),(5,3)], [(7, 1), (1, 7)])\n",
    "agent = QLearningAgent(environment, learning_rate=0.1, epsilon=0.1, discount_factor=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(episodes=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(agent.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importações necessárias\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from IPython.display import HTML\n",
    "import time\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Preparando a representação gráfica do labirinto\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Configurações de plotagem\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "\n",
    "# Função para plotar o estado do ambiente em um determinado frame\n",
    "def plot_frame(frame):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.matshow(environment.grid, cmap=plt.cm.Pastel1)\n",
    "    if frame < len(agent.path):\n",
    "        state = agent.path[frame]\n",
    "        ax.plot(state[1], state[0], 'bo', markersize=15)  # Desenha o agente\n",
    "        ax.set_title(f\"Passo {frame + 1}\")\n",
    "    # plt.xticks([]), plt.yticks([])  # Remove os ticks do eixo\n",
    "    return fig\n",
    "\n",
    "# Exibir as imagens em sequência para simular a animação\n",
    "for frame in range(len(agent.path)):\n",
    "    fig = plot_frame(frame)\n",
    "    display(fig)  # Mostra a figura\n",
    "    clear_output(wait=True)  # Limpa a saída para a próxima imagem\n",
    "    plt.close(fig)  # Fecha a figura para liberar memória\n",
    "    time.sleep(0.1)  # Velocidade da \"animação\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.q_table"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
